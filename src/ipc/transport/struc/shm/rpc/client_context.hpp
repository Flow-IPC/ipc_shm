/* Flow-IPC: Shared Memory
 * Copyright 2023 Akamai Technologies, Inc.
 *
 * Licensed under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in
 * compliance with the License.  You may obtain a copy
 * of the License at
 *
 *   https://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in
 * writing, software distributed under the License is
 * distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR
 * CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing
 * permissions and limitations under the License. */

/// @file
#pragma once

#include "ipc/transport/struc/shm/rpc/session_vat_network.hpp"
#include "ipc/session/app.hpp"
#include "ipc/common.hpp"
#include <flow/log/log.hpp>
#include <optional>
#include <algorithm>

namespace ipc::transport::struc::shm::rpc
{

// Types.

/**
 * An object of this type facilitates the establishment, in client style, of a zero-copy-enabled capnp-RPC
 * conversation with an opposing process's Context_server.  On successful connect, it represents one such
 * conversation until its destruction; create another `*this` to begin another such conversation.
 *
 * Before a successful connection (sync_connect()) a `*this` is said to be in NULL state.  After it and until
 * destruction it is said to be in PEER state.
 *
 * Once a `*this` is in a PEER state (on successful sync_connect()), its capabilities/semantics/API are exactly
 * identical to those of Server_context, objects of which type are generated by an opposing process's Context_server
 * object.  Thus the following relationships are analogous (if you're not familiar with Flow-IPC at large you may
 * disregard):
 *   - In zero-copy-enabled-RPC land:
 *     Client_context (in PEER state) - Context_server - Server_context (always in PEER state).
 *   - In general Flow-IPC land:
 *     session::Client_session (in PEER state) - session::Session_server - session::Server_session (in PEER state).
 *
 * ### Background ###
 * (This description will go down easier if you are familiar with capnp-RPC basics.)
 *
 * As introduced in ipc::transport::struc::shm::rpc namespace doc header, Flow-IPC's support zero-copy RPC
 * centers on its implementation of the `capnp::VatNetwork` interface: Session_vat_network.  Consider its
 * non-zero-copy cousin shipped with capnp, `TwoPartyVatNetwork`.  To use it -- in the absence of simplifying
 * all-in-one helpers like `capnp::EzRpcClient` -- the following essential steps are usually involved,
 * on the *client end*:
 *   -# Connect a low-level transport stream (for local IPC, a Unix domain socket stream) to a listening server
 *      at some address (in Linux typically an abstrace address; or more generally a file-addressed socket).
 *   -# Create a `TwoPartyVatNetwork`, giving it the stream endpoint (e.g., as a socket handle; from step 1),
 *      which it owns from that point on (up to and including closing it).
 *   -# Create an `RpcSystem`, passing it a reference to the `VatNetwork` from step 2; then perform RPC
 *      in various ways.  (Typically, if one connected as a client, they would also obtain an interface impl
 *      from the other side and go from there; but this can be reversed... various possibilities exist.)
 *   -# Once either this side decides to be done, or the other side closes the conversation (the latter might
 *      be detected via a `Promise` returned by `TwoPartyVatNetwork::onDisconnect()`):
 *      - Deinit in the opposite order: `RpcSystem`, `VatNetwork` (which now owns the stream).
 *
 * ### How to use Client_context ###
 * A Client_context object facilitates a very similar (almost identical) workflow.  The following points apply:
 *   - To specify the target opposing server, use the ipc::session::App system.  Don't worry: it is straightforward,
 *     yet it side-steps certain problems of portability/lack of abstraction and safety present when working with
 *     native IPC OS primitives directly.  In any case you'll setup a session::Client_app to ID this application; and
 *     session::Server_app to ID the opposing application.  (You'll give the same to Context_server on the
 *     opposing side.)
 *   - Client_context::sync_connect() will perform both steps 1 and 2 above...
 *     - ...except the `VatNetwork` -- which you should obtain via the vat_network() accessor of `*this` --
 *       is Session_vat_network.  That is, it will avoid all copying of underlying messages, meaning they can
 *       effective be of any size at all with no difference in performance/latency.  Internally it will use
 *       SHared Memory (SHM) supplied by Flow-IPC.
 *   - You specify the SHM-provider to be used (behind the scenes!) via your choice of template parameter
 *     `Client_session_t`.  As of this writing the following values are available out of the box.
 *     - (SHM-classic) session::shm::classic::rpc::Client_session.  A good default choice.
 *     - (SHM-jemalloc) session::shm::arena_lend::jemalloc::rpc::Client_session.  For more advanced considerations
 *       particularly in terms of safety (e.g., each side holds its own segregated SHM arena, and the other side
 *       may -- at a kernel level -- only read from it).  There are also perf implications (the commercial-grade
 *       memory-manager, jemalloc, is used for individual allocations).
 *     - Please see the guided Manual and/or in-code doc headers such as for namespace ipc::shm.  99.9999% of this
 *       information will have no bearing on your use of capnp-RPC, semantically, but it may eventually be
 *       significant in terms of perf and safety considerations in production code.
 *
 * ### For Flow-IPC-savvy users / Direct SHM use ###
 * The above was written with a slant toward those who are familiar with capnp-RPC but may know next to nothing
 * about Flow-IPC at large.  However Client_context does provide access to the rest of Flow-IPC my exposing
 * most (not quite all) features of ipc::session::Session.  The latter is a (the!) gateway to the rest
 * of Flow-IPC which is full-featured.  To wit:
 *   - You may obtain the Flow-IPC `Session` object, after a successful sync_connect().  Call it `S`.
 *     - `S->session_shm()` and `S->app_shm()` give you direct access to SHM arenas, where you can directly
 *       construct C++ native objects (including arbitrary STL-compliant containers-of-containers-of-...),
 *       then transmit them to the opposing process and more.  (See ipc::shm::stl namespace doc header.)
 *       - As you can see in other docs, `S->lend_object()` and `S->borrow_object()` enable sharing native SHM-stored
 *         objects between processes.  To transmit the handle blobs return by `->lend_object()`, you can use
 *         your shiny capnp-RPC session and/or any additional `Channel`s you opened via sync_connect()
 *         (or any other technique for that matter... but why would you need even more?!).
 *   - You may open direct-use channels (ipc::transport::Channel) via the optional args to sync_connect().
 *     These will be available immediately.  You can then use them directly (for low-level transport work),
 *     upgrade them to ipc::transport::struc::Channel for a simplified capnp-message exchange, and so on.
 *     See the relevant documentation.
 *     - By choosing the template parameter (which has a sensible default value also) for
 *       `"ipc::session::shm::*::Client_session"` (in setting our template-arg `Client_session_t`), you may make
 *       use of several transport mechanisms including (as of this writing) POSIX MQs, boost.interprocess (in-SHM)
 *       MQs, and socket-streams (of bytes and/or native-handles/FDs).
 *
 * Some features of ipc::session::Session are not usable through a Client_context.
 *   - The capnp-metadata exchange (ipc::session::Session::mdt_builder() et al) at session-open is not available.
 *     This would have complicated the API, and we figure the capnp-RPC layer is more than sufficient for any
 *     negotiation or information exchange you desire!
 *   - While you can call `S->open_channel()` (which would open a new `Channel` *after* the session is connected),
 *     it will not work, as there is no way to specify a passive-channel-open handler.  This would have, again,
 *     complicated the API -- and we'd want to perhaps make it available in a `kj`-style way (promises, promises).
 *     - This is a possibility for future work.  However (1) capnp-RPC is available, even for native-handle exchange
 *       if desired ("capability passing" -- see capn-RPC's `getFd()`); and (2) even for vanilla Flow-IPC work
 *       most prefer to use the at-session-open channels (which are available in Client_context), so open_channel()
 *       et al are in the first place an advanced feature.
 *
 * @internal
 *
 * @todo There is a tiny protocol involved between rpc::Client_context and rpc::Context_server, but (internally)
 * we lack a util::Protocol_negotiator; arguably we should add it, lest there is compatibility issue should we need
 * to modify this protocol.  The protocol is trivial: the agreement that the first client-requested init-channel
 * is actually for capnp-RPC purposes and not to be emitted to the `sync_connect()`/`async_accept()` invoker.
 * Without `Protocol_negotiator`, which admittedly would complicate the impl annoyingly, we can probably survive
 * if we must; e.g., by adding to the *actual* protocol used during the fairly sophisticated capnp-backed ipc::session
 * handshake over the session-master-channel.  So the to-do is arguable.
 *
 * @endinternal
 *
 * @tparam Client_session_t
 *         A concrete type a-la session::Client_session but SHM-enabled.  As of this writing
 *         out of the box the available types are listed above under "How to use `Client_context`."
 *         Naturally, the opposing Context_server must be parameterized in a compatible fashion.
 *         (E.g., `Client_context<ipc::session::shm::classic::Client_session<>>` <=>
 *                `Context_server<ipc::session::shm::classic::Session_server<>>`.)
 */
template<typename Client_session_t>
class Client_context :
  public flow::log::Log_context,
  private boost::noncopyable
{
public:
  // Types.

  /// Convenience alias for template arg.
  using Session_obj = Client_session_t;
  static_assert(Session_obj::S_SHM_ENABLED,
                "Client_context facilitates the use of Flow-IPC zero-copy (SHM) facilities for "
                  "zero-copy performance in using capnp-RPC; therefore the supplied Client_session_t template "
                  "parameter must be a SHM-enabled Client_session variant, session::shm::*::Client_session*.");

  /**
   * The Session_vat_network concrete type -- and `capnp::VatNetwork` interface impl -- that we produce.
   * See Session_vat_network docs; but in short, generally, once constructed this guy is used ~identically
   * to `capnp::TwoPartyVatNetwork`.
   */
  using Vat_network = typename Session_obj::Vat_network;

  // Constructors/destructor.

  /**
   * Constructs us without establishing the capnp-RPC session.  Use sync_connect() to establish it.
   *
   * @param logger_ptr
   *        Logger to use for logging subsequently.  (You may use null to forego this completely.)
   * @param kj_io
   *        A `kj` event loop context.
   * @param cli_app_ref
   *        Properties of this client application.  The address is copied; the object is not copied.
   * @param srv_app_ref
   *        Properties of the opposing server application.  The address is copied; the object is not copied.
   * @param enable_hndl_transport
   *        `true` means native handles (a/k/a capabilities in capnp-RPC parlance) can
   *        be RPC-transmitted (i.e., your interface impls and clients can use `.getFd()` and yield something
   *        as opposed to nothing).  `false` means the opposite; in particular any native-handles arriving via
   *        capnp-RPC (as enabled via the #Vat_network returned by vat_network()) shall be disregarded.
   *        If you do not use `.getFd()` feature of the capnp-RPC system, it is best to set this to `false`,
   *        it is said, for safety and possibly even security.
   */
  explicit Client_context(flow::log::Logger* logger_ptr, kj::AsyncIoContext* kj_io,
                          const session::Client_app& cli_app_ref, const session::Server_app& srv_app_ref,
                          bool enable_hndl_transport = true);

  /**
   * Destructor; in particular (assuming sync_connect() succeeded) shuts down the #Vat_network and the
   * #Session_obj, in that order.  You must destroy any related #Rpc_system before invoking this dtor.
   * If you have any directly-obtained (via `->construct<T>()` or `->borrow_object<T>()`) SHM-handles from
   * `*this`, you must have nullified them before invoking this dtor.
   *
   * Informally we recommend, also, that once you disconnect your capnp-RPC conversation, or
   * the other side does so -- this can be detected via `vat_network()->on_disconnect()`-returned `kj::Promise` --
   * you destroy `*this` sans delay.  (Do not worry: there is no race condition.  But we digress.)
   */
  ~Client_context();

  // Methods.

  /**
   * To be invoked successfully at most once, it synchronously and non-blockingly attempts to
   * connect to an opposing `Context_server`; and synchronously reports failure or success, the latter
   * showing `*this` has entered PEER state.  Failure means `*this` remains in NULL state.
   *
   * Note that `*this` that has entered PEER state can never change state subsequently
   * (even on transmission error); once a PEER, always a PEER.
   *
   * Essentially this convenience operation has one main aspect:
   *   - It bundles together the initiation of a Flow-IPC session::Session and the creation of a `capnp::VatNetwork`
   *     (similar to a vanilla `capnp::TwoPartyVatNetwork` but zero-copy-enabled using Flow-IPC SHM resources from
   *     said `Session`).  So: one immediate op; and then you can make an `RpcSystem` and go capnp-RPC-ing happily; and
   *     (optionally) make use of certain Flow-IPC `Session` features as well (see below and class doc header).
   *
   * ### How does it know "where" the Session_server is listening? / Why non-blocking and synchronous? ###
   * Please see session::Client_session::sync_connect() doc header's similar section.  It is best to be aware
   * of this as background.
   *
   * ### Error conditions ###
   * If invoked already in PEER state, this returns `false` and otherwise does nothing.  The following assumes
   * otherwise (that it was invoked properly in NULL state).  Then:
   *
   * On success returns `true`.  On connect fail, throws an exception `flow::error::Runtime_error`.  The `Error_code`
   * may be accessed via `.code()` (also `.code().message()`, `.what()`) of the exception object.
   * #Error_code generated:
   *   - interprocess-mutex-related errors (probably from boost.interprocess) w/r/t reading the CNS (PID file),
   *     file-related system errors w/r/t reading the CNS (PID file) (see Session_server doc header for background),
   *   - error::Code::S_CLIENT_NAMESPACE_STORE_BAD_FORMAT (bad CNS contents),
   *     those emitted by transport::Native_socket_stream::sync_connect(),
   *     those emitted by transport::struc::Channel::send(),
   *     those emitted by transport::struc::Channel via on-error handler (most likely
   *     transport::error::Code::S_RECEIVES_FINISHED_CANNOT_RECEIVE indicating graceful shutdown of opposing process
   *     coincidentally during log-in procedure, prematurely ending session while it was starting),
   *     error::Code::S_CLIENT_MASTER_LOG_IN_RESPONSE_BAD,
   *     error::Code::S_INVALID_ARGUMENT (other side expected other sync_connect() overload with
   *     non-null `init_channels_by_srv_req` arg),
   *     session::error::Code::S_SESSION_OPEN_CHANNEL_SERVER_CANNOT_PROCEED_RESOURCE_UNAVAILABLE (SHM-jemalloc
   *     internal-use socket-connection could not be obtained from OS by opposing side),
   *   - possibly others.
   *
   * `this->sync_connect()` may retried, if an exception is thrown.
   *
   * @note To comport with capnp-RPC/`kj` style code flow, this API does not have a standard Flow-style
   *       optional `Error_code*` out-arg through which to communicate success/errors instead of exceptions if desired;
   *       it shall always fire an exception on error, otherwise return `true`.
   *
   * ### Seriously, isn't it common to be able to perform a socket-async-connect technique? ###
   * In a local-IPC context: In short, actually, no.  (Even in a networked context the potential asynchronicity --
   * waiting -- really comes from the fact that connection establishment with a *listening* server involves
   * networked round trips which are not instant; not the case here.)
   *
   * There is however a to-do elsewhere (which would propagate here as well) for a relatively exotic
   * wait-until-server-pops-up operation.
   *
   * @param init_channels_by_cli_req_pre_sized
   *        Null or pointer to container of `Session_obj::Channel_obj` with `.size()` specifying how many channels
   *        this side is requesting to be opened on its behalf; each element will be move-assigned
   *        a PEER-state `Channel_obj` on success.  Recommend simply ct-ing with `(n)` or `.resize(n)` which
   *        loads it with default-cted (NULL-state) objects to be replaced.  null is treated same as
   *        `.empty()`.
   * @param init_channels_by_srv_req
   *        Null or pointer to container of `Channel_obj` which shall be `.clear()`ed and replaced
   *        by a container of PEER-state `Channel_obj` on success the number being specified by the opposing
   *        (server) side.  The number may be zero.  null is allowed if and only if the number is zero;
   *        otherwise error::Code::S_INVALID_ARGUMENT is emitted.
   * @return `true` on successful connect; `false` if invoked in PEER state (already connected).
   *         Exception thrown on failure to connect (`.code()` indicating why).
   */
  bool sync_connect(typename Session_obj::Channels* init_channels_by_cli_req_pre_sized = nullptr,
                    typename Session_obj::Channels* init_channels_by_srv_req = nullptr);

  /**
   * Identical to sync_connect() with same signature but disables zero-copy transport.
   * That is underlying messages are sent in the mode compatible with vanilla capnp-RPC.
   * This requires some knobs to be set appropriately at the opposing process.  To wit:
   *   - If using Ez_rpc_server: use ctor that takes `transport_method_func` arg; it must return `false`,
   *     if the session::Client_app arg equals the `cli_app_ref` passed to `*this` ctor.
   *   - If using Context_server: use Context_server::accept() that takes `transport_method_func` arg;
   *     it must (...see previous bullet...).
   *   - If using Session_vat_network: use the "special mode" ctor arg (see Session_vat_network ctor doc headers).
   *   - Or use `capnp::VatNetwork` or `capnp::EzRpcServer`.
   *
   * @param init_channels_by_cli_req_pre_sized
   *        See sync_connect().
   * @param init_channels_by_srv_req
   *        See sync_connect().
   * @return See sync_connect().
   */
  bool sync_connect_sans_shm_transport(typename Session_obj::Channels* init_channels_by_cli_req_pre_sized = nullptr,
                                       typename Session_obj::Channels* init_channels_by_srv_req = nullptr);

  /**
   * Returns pointer to Session_vat_network established for use in your rpc::Rpc_system; or null if
   * no sync_connect() has succeeded or been invoked at all.  The #Vat_network is valid if and only if
   * `*this` exists.
   *
   * See Session_vat_network docs; but in short, generally, once constructed this guy is used ~identically
   * to `capnp::TwoPartyVatNetwork`.
   *
   * @return See above.
   */
  Vat_network* vat_network();

  /**
   * Immutable counterpart to the other overload.
   * @return See above.
   */
  const Vat_network* vat_network() const;

  /**
   * Returns pointer to #Session_obj, if sync_connect() has succeeded; or null if no sync_connect() has succeeded
   * or been invoked at all.  The #Session_obj is valid if and only if `*this` exists.
   *
   * @see class doc header for overview of which features accessible through this #Session_obj are available
   *      (and which are not).
   *
   * @return See above.
   */
  Session_obj* session();

  /**
   * Immutable counterpart to the other overload.
   * @return See above.
   */
  const Session_obj* session() const;

private:
  // Methods.

  /**
   * Session-hosed handler for #m_session.  This only logs as opposed to reporting to the user of `*this` for
   * reasons explained inside (this is of some importance).
   *
   * @param err_code
   *        Reason session was hosed as reported by ipc::session module.
   */
  void on_session_hosed(const Error_code& err_code);

  /**
   * Impl of sync_connect() and sync_connect_sans_shm_transport().
   * @param init_channels_by_cli_req_pre_sized
   *        See `sync_connect*()`.
   * @param init_channels_by_srv_req
   *        See `sync_connect*()`.
   * @param sans_shm_transport
   *        Which method are we impl-ing?
   * @return See `sync_connect*()`.
   */
  bool sync_connect_impl(typename Session_obj::Channels* init_channels_by_cli_req_pre_sized,
                         typename Session_obj::Channels* init_channels_by_srv_req,
                         bool sans_shm_transport);

  // Data.

  /// `kj` event loop context.
  kj::AsyncIoContext* const m_kj_io;

  /// See ctor.  Stored in case we must re-initialize #m_session, if sync_connect() only partially succeeds.
  const session::Client_app& m_cli_app_ref;
  /// See ctor.  Stored in case we must re-initialize #m_session, if sync_connect() only partially succeeds.
  const session::Server_app& m_srv_app_ref;

  /// See ctor.
  const bool m_enable_hndl_transport;

  /// Session in NULL state (until sync_connect() succeeds) or PEER state (subsequently).
  Session_obj m_session;

  /// Null (until sync_connect() succeeds) or the established #Vat_network (subsequently).
  std::optional<Vat_network> m_network;
}; // class Client_context

// Template implementations.

template<typename Client_session_t>
Client_context<Client_session_t>::Client_context(flow::log::Logger* logger_ptr,
                                                 kj::AsyncIoContext* kj_io,
                                                 const session::Client_app& cli_app_ref,
                                                 const session::Server_app& srv_app_ref,
                                                 bool enable_hndl_transport) :
  flow::log::Log_context(logger_ptr, Log_component::S_RPC),
  m_kj_io(kj_io),
  m_cli_app_ref(cli_app_ref),
  m_srv_app_ref(srv_app_ref),
  m_enable_hndl_transport(enable_hndl_transport),
  m_session(get_logger(), m_cli_app_ref, m_srv_app_ref,
            [this](const Error_code& err_code) { on_session_hosed(err_code); })
{
  FLOW_LOG_INFO("rpc::Client_ctx [" << *this << "]: "
                "Created in NULL state (inactive).  Inactive ipc::session::Session [" << m_session << "] exists.");
}

template<typename Client_session_t>
Client_context<Client_session_t>::~Client_context()
{
  FLOW_LOG_INFO("rpc::Client_ctx [" << *this << "]: "
                "Shutting down.  Session_vat_network shall shut down; then the ipc::session::Session.");
}

template<typename Client_session_t>
void Client_context<Client_session_t>::on_session_hosed(const Error_code& err_code)
{
  FLOW_LOG_INFO("rpc::Client_ctx [" << *this << "]: "
                "ipc::session::Session [" << m_session << "] session-hosed handler fired "
                "(code [" << err_code << "] [" << err_code.message() << "]).  This is likely normal, as the "
                "opposing process decided to end session; the RPC-system will have detected same, and user "
                "RPC session should be ended or ending imminently, at which point proper user code shall "
                "shut-down this Client_ctx which will shut down the Session_vat_network and lastly the "
                "ipc::session::Session.");

  /* Why do we merely log but in no way report this to `*this` user?
   * (After all with a vanilla Flow-IPC session we make a big deal of the session-hosed handler one must pass
   * to its ctor.)  Answer: The usefulness of the handler is, outside of emitting a reason error-code (which
   * actually is not *that* interesting typically; simply the other side decided to end the session), to *signal*
   * to the user that they cannot carry on.  Moreover, excepting crashes and the like, this is not a catastrophe:
   * it means the user *should* destroy the session and everything related to it, ideally soon; but behavior
   * remains not ill-defined until then.  (Details omitted; in short for some SHM-types this requires that each
   * of two opposing destructors waits until the other begins executing, before it can execute; for others this is not
   * necessary.)
   *
   * With a general session, users may generate `Channel`s which are independent of the session in terms of their
   * lifetimes -- even though informally it is recommended to follow the best practice of init/de-init order
   * (A up, B up, B down, A down), it is not required.  So formally speaking it makes sense to have a session-end
   * signal separately from channel-end signal(s).  In our case, however, we have a *mandatory* capnp-RPC session
   * we start; really it's the main point of a `*this` after all; and *that* system already provides a natural
   * signal as to the other side being done -- which is customary for capnp-RPC users to use.  That can be something
   * in their own protocol, or they can use vat_network()->on_disconnect() promise.  We indicate this explicitly
   * in the dtor doc header for instance.
   *
   * Therefore supplying a signal through the Flow-IPC session as well is an unnecessary complication of the API --
   * an asynchronous one at that which is particularly annoying.  It becomes ambiguous as to how to handle
   * disconnection.  So let's not make it hard. */
} // Client_context::on_session_hosed()

template<typename Client_session_t>
bool Client_context<Client_session_t>::sync_connect
       (typename Session_obj::Channels* init_channels_by_cli_req_pre_sized,
        typename Session_obj::Channels* init_channels_by_srv_req)
{
  return sync_connect_impl(init_channels_by_cli_req_pre_sized, init_channels_by_srv_req, false);
}

template<typename Client_session_t>
bool Client_context<Client_session_t>::sync_connect_sans_shm_transport
       (typename Session_obj::Channels* init_channels_by_cli_req_pre_sized,
        typename Session_obj::Channels* init_channels_by_srv_req)
{
  return sync_connect_impl(init_channels_by_cli_req_pre_sized, init_channels_by_srv_req, true);
}

template<typename Client_session_t>
bool Client_context<Client_session_t>::sync_connect_impl
       (typename Session_obj::Channels* init_channels_by_cli_req_pre_sized,
        typename Session_obj::Channels* init_channels_by_srv_req,
        bool sans_shm_transport)
{
  using Channels = typename Session_obj::Channels;

  if (m_network)
  {
    FLOW_LOG_WARNING("rpc::Client_ctx [" << *this << "]: capnp-RPC connect requested, but we are already connected.  "
                     "Bug?  Ignoring.");
    return false;
  }
  // else

  /* We request 1 channel for capnp-RPC; plus any they might want for their own purposes.
   * Naturally the Session_context knows this agreement.
   * (See @todo in class doc header regarding protocol negotiation though.) */
  Channels actual_init_channels_by_cli{1 + (init_channels_by_cli_req_pre_sized
                                              ? init_channels_by_cli_req_pre_sized->size()
                                              : 0)};
  FLOW_LOG_INFO("rpc::Client_ctx [" << *this << "]: capnp-RPC connected requested.  Synchronous, non-blocking "
                "ipc::session::Session connect initiating; on success will hook up the "
                "[zero-copy-enabled? = [" << (!sans_shm_transport) << "]] Session_vat_network.");
#ifndef NDEBUG
  const bool ok =
#endif
  m_session.sync_connect(m_session.mdt_builder(), &actual_init_channels_by_cli, nullptr, init_channels_by_srv_req);
  assert(ok && "We had a NULL-state Client_session and tried .sync_connect(); why would it have reported otherwise?");

  /* .sync_connect() would have thrown on error (as advertised) (probably other guy isn't up; m_network remained null).
   * Otherwise we carry on: */

  if (init_channels_by_cli_req_pre_sized && (!init_channels_by_cli_req_pre_sized->empty()))
  {
    assert(actual_init_channels_by_cli.size() > 1);
    std::move(++actual_init_channels_by_cli.begin(), actual_init_channels_by_cli.end(),
              init_channels_by_cli_req_pre_sized->begin());
    actual_init_channels_by_cli.resize(1);
  }
  assert(actual_init_channels_by_cli.size() == 1);
  auto& chan_for_rpc = actual_init_channels_by_cli.front();

  try
  {
    if (m_enable_hndl_transport)
    {
      // Let it use the sensible default for last arg.
      m_network.emplace(get_logger(), m_kj_io, sans_shm_transport ? nullptr : &m_session,
                        &chan_for_rpc);
    }
    else
    {
      // As requested disable handle-transmission (last arg).
      m_network.emplace(get_logger(), m_kj_io, sans_shm_transport ? nullptr : &m_session,
                        &chan_for_rpc, 0);
    }
  }
  catch (...)
  {
    FLOW_LOG_WARNING("rpc::Client_ctx [" << *this << "]: ipc::session::Session [" << m_session << "] connected "
                     "fine; however Session_vat_network threw exception which we shall re-throw; NULLifying "
                     "Session first to get `*this` back to pristine state.");
    m_session = Session_obj(get_logger(), m_cli_app_ref, m_srv_app_ref,
                            [this](const Error_code& err_code) { on_session_hosed(err_code); });
    assert((!m_network) && "Session_vat_network ctor would have thrown -- m_network should still be null!");

    throw;
  }
  // Got here: noice!  (m_network ctor logged just fine, so let's not.)

  // chan_for_rpc will go away here.  That is fine; its useful part is transferred into *m_network anyway.

  return true;
} // Client_context::sync_connect()

template<typename Client_session_t>
typename Client_context<Client_session_t>::Vat_network*
  Client_context<Client_session_t>::vat_network()
{
  return m_network ? &(*m_network) : nullptr;
}

template<typename Client_session_t>
const typename Client_context<Client_session_t>::Vat_network*
  Client_context<Client_session_t>::vat_network() const
{
  return const_cast<Client_context*>(this)->vat_network(); // Rare use of const_cast<> that's not an anti-pattern.
}

template<typename Client_session_t>
typename Client_context<Client_session_t>::Session_obj*
  Client_context<Client_session_t>::session()
{
  return m_network ? &m_session : nullptr;
}

template<typename Client_session_t>
const typename Client_context<Client_session_t>::Session_obj*
  Client_context<Client_session_t>::session() const
{
  return const_cast<Client_context*>(this)->session(); // Rare use of const_cast<> that's not an anti-pattern.
}

template<typename Client_session_t>
std::ostream& operator<<(std::ostream& os, const Client_context<Client_session_t>& val)
{
  const auto network = val.vat_network();
  os << '[';
  if (network)
  {
    os << "session[" << *(val.session()) << "] vat_netwk[" << *network << ']';
  }
  else
  {
    os << "null";
  }
  return os << "]@" << &val;
}

} // namespace ipc::transport::struc::shm::rpc
